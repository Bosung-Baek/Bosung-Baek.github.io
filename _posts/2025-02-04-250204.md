---
layout: single
title: "Frequency Attention for Knowledge Distillation
(WACV 2024)"
date: "2025-02-05"
comments: "true"
---

# Frequency Attention for Knowledge Distillation

## 1. Introduction

### 배경
- Knowledge Distillation (KD)는 경량화된 딥러닝 모델을 학습하는 기법으로, 큰 Teacher 모델의 지식을 작은 Student 모델로 전이.
- 기존 Attention 기반 KD는 대부분 **공간 영역(spatial domain)** 에서 수행되어 국소적(local) 정보에 초점.
- **주파수 영역(frequency domain)** 은 전체 픽셀을 기반으로 전역(global) 정보를 포함할 수 있음.

### 연구 목표
- 주파수 영역에서 동작하는 새로운 **Frequency Attention Module (FAM)** 을 제안.
- FAM을 Layer-to-Layer KD 및 Knowledge Review KD에 적용하여 성능 향상.

## 2. Related Work

### 기존 KD 기법
1. **Logit 기반 KD**: Softmax 출력을 정제하여 distillation 수행.
2. **Feature 기반 KD**: 중간 feature map을 활용한 KD.
3. **Attention 기반 KD**: Spatial attention을 사용하여 중요한 영역에 집중.

### 주파수 영역에서의 KD 필요성
- 주파수 영역은 이미지 내 반복적 패턴과 전역 구조를 강조할 수 있음.
- 기존 Attention 기반 KD는 공간적(local) 변화만 반영하여 전역 정보 손실 발생.
- 주파수 영역에서의 Attention은 전체적인 이미지 정보를 반영하여 보다 효과적인 지식 전이를 가능하게 함.

## 3. Proposed Method

### Frequency Attention Module (FAM)
- FAM은 **전역(global) 및 국소(local) branch** 로 구성됨.
- 전역 branch는 **Fast Fourier Transform (FFT)** 를 통해 feature를 주파수 영역으로 변환.
- **Learnable Global Filter** 를 통해 teacher의 특징을 반영.
- **High Pass Filter (HPF)** 를 적용하여 저주파수 성분을 억제하여 중요한 고주파 특징 강조.
- Inverse FFT를 사용하여 다시 공간 영역으로 변환.
- 국소 branch는 1x1 convolution을 사용하여 공간적 정보 보존.
- 최종적으로 두 branch를 가중합하여 학습 수행.

#### 수식 정의
- FFT 변환:\

$$
  X(u, v) = \sum_{k=0}^{H-1} \sum_{l=0}^{W-1} X(k, l) e^{-i 2\pi (uk/H + vl/W)} 
$$

- IFFT 변환:\

$$
  X(k, l) = \frac{1}{HW} \sum_{u=0}^{H-1} \sum_{v=0}^{W-1} X(u, v) e^{i 2\pi (uk/H + vl/W)} 
$$

### FAM의 KD 적용 방식
1. **Layer-to-Layer KD**
   - Student 모델의 중간 feature를 FAM을 통해 변환하여 teacher의 feature에 근접하도록 학습.
   - 추가적으로 **Local Self-Attention (LA)** 을 사용하여 공간 영역에서의 구조 강화.

2. **Knowledge Review KD**
   - Teacher의 저수준(low-level) feature를 활용하여 student의 심층 feature를 보정.
   - Feature Fusion을 위해 **Cross-Attention** 적용.
   - FAM을 활용하여 전역 주파수 정보를 반영한 Knowledge Review.

## 4. Experiments

### 실험 데이터셋
- **CIFAR-100**, **ImageNet** (이미지 분류)
- **MS COCO** (객체 검출)

### 주요 실험 결과
#### CIFAR-100
- Layer-to-Layer KD: 기존 기법보다 높은 성능.
- Knowledge Review KD: 기존 ReviewKD보다 향상된 성능.

#### ImageNet
- ResNet34 → ResNet18, ResNet50 → MobileNetV1 설정에서 기존 SOTA 대비 1% 이상의 성능 향상.

#### MS COCO (객체 검출)
- Faster R-CNN + FPN 모델에서 FAM-KD 적용 시, 기존 DKD + ReviewKD 대비 성능 향상.

#### 시각적 비교
- Grad-CAM 기반 **Feature Map Visualization** 비교 이미지 포함 (논문에서 제공된 그림 추가 필요).

### Ablation Study
- **Global vs. Local Branch**: 전역 branch가 중요한 역할 수행.
- **High Pass Filter (HPF)**: HPF를 제거하면 성능 저하.

## 5. Conclusion
- **주파수 영역에서 동작하는 새로운 FAM 모듈 제안**.
- **Layer-to-Layer KD 및 Knowledge Review KD에 적용하여 성능 향상 확인**.
- **다양한 데이터셋에서 SOTA 대비 우수한 성능**을 보임.
- 향후 연구 방향: 다양한 주파수 변환 기법 및 Transformer 기반 모델과의 결합.

## 6. References
- 논문에 포함된 참고 문헌 정리

