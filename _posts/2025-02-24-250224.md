---
layout: single
title: "Reciprocal Teacher-Student Learning via Forward and Feedback Knowledge Distillation (IEEE TMM 2024)"
date: "2025-02-24"
comments: "true"
use_math: true
categories: 
  - Paper-Review
sitemap:
  changefreq: daily
  priority : 1.0
---


# 논문 리뷰: Reciprocal Teacher-Student Learning via Forward and Feedback Knowledge Distillation

---

## 📌 목차
1. [논문 정보](#-논문-정보)
2. [개요 (Abstract)](#-1-개요-abstract)
3. [배경 (Background)](#-2-배경-background)
4. [제안 방법 (Method)](#-3-제안-방법-method)
5. [실험 (Experiments)](#-4-실험-experiments)
6. [결론 및 아이디어 (Conclusion & Ideas)](#-5-결론-및-아이디어-conclusion--ideas)

---

## 📌 논문 정보
- **제목**: Reciprocal Teacher-Student Learning via Forward and Feedback Knowledge Distillation
- **저자**: Jianping Gou, Yu Chen, Baosheng Yu, Jinhua Liu, Lan Du, Shaohua Wan, Zhang Yi
- **학회**: IEEE Transactions on Multimedia, 2024
- **DOI**: 10.1109/TMM.2024.3372833

---



## 📌 1. 개요 (Abstract)
### ✅ 연구 배경
- **지식 증류(Knowledge Distillation, KD)** 는 대형 모델(Teacher)의 지식을 작은 모델(Student)로 전이하여 모델 경량화를 가능하게 함.
- 기존 KD 방법들은 **일방향 전이(One-way transfer)** 방식으로, Teacher가 Student에게 고정된 지식을 전달하는 구조가 대부분.
- 하지만 **Teacher와 Student의 성능 격차(capacity gap)로 인해 Student가 모든 지식을 효과적으로 학습하지 못하는 문제**가 발생.
- 또한, **기존 온라인 KD 방식에서는 Teacher의 부재로 인해 Student 간 지식 불일치 문제**가 발생할 수 있음.

### ✅ 연구 목표
- **Teacher와 Student 간 쌍방향 학습(Reciprocal Learning) 구조를 도입하여 지식 증류 성능을 향상**.
- **Forward Knowledge Distillation (FKD)**: 기존 KD처럼 Teacher가 Student에게 지식을 전달.
- **Feedback Knowledge Distillation (FBKD)**: Student가 학습한 내용을 Teacher에게 피드백하여 Teacher가 가르치는 방식을 개선.
- **중간 피처 지식 교환**을 통해 Teacher가 더 나은 Teaching 전략을 학습.
- **기능 유사도 평가를 위한 Center Kernel Alignment (CKA) 활용**.

---

## 📌 2. 배경 (Background)
### 🔹 기존 KD 방식
1. **Offline KD**: Pre-trained Teacher가 Student에게 일방적으로 지식을 전달.
2. **Online KD**: 여러 Student 간 협력 학습(peer teaching) 구조를 이용하지만 Teacher 부재로 성능이 제한적.

### 🔹 기존 방법의 한계
- **Teacher가 가르치는 내용이 항상 최적이 아님** → 학생의 학습 상태를 반영하지 않음.
- **Student가 이해하지 못한 부분을 Teacher가 인지하지 못함** → 효과적인 피드백 없음.
- **Student 간 협력 학습의 한계** → Teacher가 없으면 올바른 지식을 정제하기 어려움.

### 🔹 본 논문의 차별점
- **Forward & Feedback Knowledge Distillation (FFKD)** 구조 도입.
- **Student의 피드백을 Teacher가 반영하여 Teaching 전략 최적화**.
- **CKA 기반 Feature Similarity 평가 기법 적용**.

---

## 📌 3. 제안 방법 (Method)
### ✅ **FFKD 개요**
FFKD는 두 단계의 학습을 포함함.
1. **Forward Knowledge Distillation (FKD)**
   - Teacher가 Student에게 기본적인 지식을 전달.
   - CKA를 사용하여 Feature Representation 유사도를 측정하여 학생의 이해도를 평가.
   - KL Divergence 기반 지식 전이 수행.

2. **Feedback Knowledge Distillation (FBKD)**
   - Student가 Teacher에게 피드백을 제공하여 Teacher의 Teaching 전략을 업데이트.
   - Student가 이해하지 못한 부분을 Teacher가 조정할 수 있도록 CKA 기반 손실 함수를 적용.

### ✅ **중요한 핵심 요소**
1. **CKA 기반 Feature Similarity 평가**
   - Teacher와 Student 간의 Feature Map 유사도를 측정하여 학습 진척도를 평가.
   - 수식:
     \[
     \omega( U_T, V_S ) = \frac{H( U_T, V_S )}{\sqrt{H( U_T, U_T ) H( V_S, V_S )}}
     \]
     - \( U_T, V_S \) : Teacher와 Student의 Feature Map
     - \( H \) : Hilbert-Schmidt Independence Criterion

2. **채널 가중치 적용(Channel Weighting)**
   - CKA를 사용하여 특정 채널의 중요도를 조정하여 효과적인 Feature Distillation 수행.

3. **Feedback 기반 Teacher 업데이트**
   - Student의 학습 상태를 반영하여 Teacher의 가르치는 방식을 최적화.

---

## 📌 4. 실험 (Experiments)
### ✅ **실험 설정**
- **데이터셋**: CIFAR-100, Tiny-ImageNet, ImageNet, CUB200-2011, Cars196
- **모델 아키텍처**: ResNet18, ShuffleNetV2, MobileNetV2, VGG
- **비교 방법**:
  - 기존 KD 방법 (KD, CD, CCS, ICKD, SPKD)
  - 기존 온라인 KD 방법 (DML, KDCL, SOKD)

### ✅ **주요 결과**

| 방법 | CIFAR-100 Top-1(%) | Tiny-ImageNet Top-1(%) |
|--------|----------------|------------------|
| Standard KD | 74.3 | 58.1 |
| CCS | 75.5 | 59.3 |
| ICKD | 76.0 | 60.1 |
| **FFKD (Ours)** | **78.2** | **62.5** |

- **FFKD는 기존 방법보다 2~4% 높은 성능 향상**을 보임.
- **Teacher도 성능이 향상됨** → Feedback을 통해 Teaching 전략이 최적화되었기 때문.

### ✅ **Ablation Study 결과**

| 설정 | CIFAR-100 Top-1(%) |
|--------|----------------|
| 기본 KD (FKD Only) | 74.3 |
| FKD + CKA 적용 | 76.1 |
| FKD + FBKD (Feedback 추가) | 77.5 |
| **FFKD (FKD + FBKD + CKA)** | **78.2** |

- **Feedback Knowledge Distillation이 성능 향상에 기여**.
- **CKA 기반 채널 가중치 적용 시 성능 추가 개선**.

---

## 📌 5. 결론 및 아이디어 (Conclusion & Ideas)
### ✅ **결론**
- FFKD는 **Teacher와 Student 간 Reciprocal Learning 구조를 도입하여 지식 증류를 최적화**.
- **CKA 기반 Feature Similarity를 활용하여 효과적인 Feature Distillation 수행**.
- **Feedback을 통해 Teacher가 지속적으로 Teaching 전략을 업데이트할 수 있도록 설계**.
- 기존 지식 증류 기법 대비 **2~4% 성능 향상**을 보이며, 특히 작은 모델에서도 효과적임.

### 🚀 **향후 연구 방향**
- **Transformer 기반 모델에 적용 가능성 탐색** (예: ViT, Swin Transformer)
- **다양한 Vision Task (Object Detection, Segmentation)로 확장 연구**
- **Siamese Network 및 Contrastive Learning과 결합하여 활용 가능성 검토**

---

✍️ **Written by Bosung Baek**

